# -*- coding: utf-8 -*-
# @Project: index_eab
# @Module: heu_run
# @Author: Wei Zhou
# @Time: 2023/8/16 14:42

import configparser
import json

from index_advisor_selector.index_selection.heu_selection.heu_algos.anytime_algorithm import (
    AnytimeAlgorithm,
)
from index_advisor_selector.index_selection.heu_selection.heu_algos.auto_admin_algorithm import (
    AutoAdminAlgorithm,
)
from index_advisor_selector.index_selection.heu_selection.heu_algos.cophy_algorithm import (
    CoPhyAlgorithm,
)
from index_advisor_selector.index_selection.heu_selection.heu_algos.db2advis_algorithm import (
    DB2AdvisAlgorithm,
    IndexBenefit,
)
from index_advisor_selector.index_selection.heu_selection.heu_algos.drop_algorithm import (
    DropAlgorithm,
)
from index_advisor_selector.index_selection.heu_selection.heu_algos.extend_algorithm import (
    ExtendAlgorithm,
)
from index_advisor_selector.index_selection.heu_selection.heu_algos.relaxation_algorithm import (
    RelaxationAlgorithm,
)
from index_advisor_selector.index_selection.heu_selection.heu_utils import heu_com
from index_advisor_selector.index_selection.heu_selection.heu_utils.heu_com import (
    get_parser,
)
from index_advisor_selector.index_selection.heu_selection.heu_utils.index import Index
from index_advisor_selector.index_selection.heu_selection.heu_utils.postgres_dbms import (
    PostgresDatabaseConnector,
)
from index_advisor_selector.index_selection.heu_selection.heu_utils.workload import (
    Workload,
)

ALGORITHMS = {
    "auto_admin": AutoAdminAlgorithm,
    "db2advis": DB2AdvisAlgorithm,
    "drop": DropAlgorithm,
    "extend": ExtendAlgorithm,
    "relaxation": RelaxationAlgorithm,
    "anytime": AnytimeAlgorithm,
    "cophy": CoPhyAlgorithm,
}


class IndexEncoder(json.JSONEncoder):
    def default(self, obj):
        # ğŸ‘‡ï¸ if passed in object is instance of Decimal
        # convert it to a string

        # db2advis
        if isinstance(obj, IndexBenefit):
            return str(obj)
        if isinstance(obj, Index):
            return str(obj)
        if "Workload" in str(obj.__class__):
            return str(obj)
        if "Index" in str(obj.__class__):
            return str(obj)
        if "Column" in str(obj.__class__):
            return str(obj)
        if isinstance(obj, set):
            return list(obj)

        # ğŸ‘‡ï¸ otherwise use the default behavior
        return json.JSONEncoder.default(self, obj)


def get_heu_result(args, algos, work_list):
    print(f"å¼€å§‹å¤„ç†ç®—æ³•: {algos}")
    print(
        f"å·¥ä½œè´Ÿè½½åŒ…å« {len(work_list) if hasattr(work_list, '__len__') else 'N/A'} ä¸ªæŸ¥è¯¢"
    )

    print("è¯»å–æ•°æ®åº“é…ç½®æ–‡ä»¶...")
    db_conf = configparser.ConfigParser()
    db_conf.read(args.db_conf_file)

    print(f"ä»æ¨¡å¼æ–‡ä»¶è·å–åˆ—ä¿¡æ¯: {args.schema_file}")
    _, columns = heu_com.get_columns_from_schema(args.schema_file)
    print(f"è·å–åˆ° {len(columns)} ä¸ªåˆ—")

    print("å»ºç«‹æ•°æ®åº“è¿æ¥...")
    connector = PostgresDatabaseConnector(
        db_conf,
        autocommit=True,
        host=args.host,
        port=args.port,
        db_name=args.db_name,
        user=args.user,
        password=args.password,
    )
    print("æ•°æ®åº“è¿æ¥å»ºç«‹æˆåŠŸ")

    res_data = dict()
    for algo in algos:
        print(f"å¼€å§‹å¤„ç†ç®—æ³•: {algo}")
        # indexes, no_cost, total_no_cost, ind_cost, total_ind_cost, sel_info
        exp_conf_file = args.exp_conf_file.format(algo)
        print(f"è¯»å–å®éªŒé…ç½®æ–‡ä»¶: {exp_conf_file}")

        with open(exp_conf_file, "r") as rf:
            exp_config = json.load(rf)

        configs = heu_com.find_parameter_list(
            exp_config["algorithms"][0], params=args.sel_params
        )
        print(f"æ‰¾åˆ° {len(configs)} ä¸ªé…ç½®")

        # (0824): newly modified.
        print("åˆ›å»ºå·¥ä½œè´Ÿè½½å¯¹è±¡...")
        workload = Workload(
            heu_com.read_row_query(
                work_list,
                exp_config,
                columns,
                type="",
                varying_frequencies=args.varying_frequencies,
                seed=args.seed,
            )
        )
        print(f"å·¥ä½œè´Ÿè½½åŒ…å« {len(workload.queries)} ä¸ªæŸ¥è¯¢")

        data = list()
        for config_idx, config in enumerate(configs, 1):
            print(f"å¤„ç†é…ç½® {config_idx}/{len(configs)}: {config['parameters']}")

            print("åˆ é™¤å‡è®¾ç´¢å¼•...")
            connector.drop_hypo_indexes()

            # (0818): newly added.
            if args.constraint is not None:
                config["parameters"]["constraint"] = args.constraint
                print(f"è®¾ç½®çº¦æŸæ¡ä»¶: {args.constraint}")
            if args.budget_MB is not None:
                config["parameters"]["budget_MB"] = args.budget_MB
                print(f"è®¾ç½®å­˜å‚¨é¢„ç®—: {args.budget_MB} MB")
            if args.max_indexes is not None:
                config["parameters"]["max_indexes"] = args.max_indexes
                print(f"è®¾ç½®æœ€å¤§ç´¢å¼•æ•°: {args.max_indexes}")

            # (0926): newly added.
            if "max_index_width" in args and args.max_index_width is not None:
                config["parameters"]["max_index_width"] = args.max_index_width
                print(f"è®¾ç½®æœ€å¤§ç´¢å¼•å®½åº¦: {args.max_index_width}")

            # (0918): newly added.
            if algo == "drop" and "multi_column" in args:
                config["parameters"]["multi_column"] = args.multi_column
                print(f"è®¾ç½®å¤šåˆ—ç´¢å¼•: {args.multi_column}")

            # (1211): newly added. for `cophy`
            if algo == "cophy":
                config["parameters"]["ampl_bin_path"] = args.ampl_bin_path
                config["parameters"]["ampl_mod_path"] = args.ampl_mod_path
                config["parameters"]["ampl_dat_path"] = args.ampl_dat_path
                config["parameters"]["ampl_solver"] = args.ampl_solver
                print("é…ç½®CoPhYç®—æ³•çš„AMPLå‚æ•°")

            print(f"åˆå§‹åŒ–{algo}ç®—æ³•...")
            algorithm = ALGORITHMS[algo](
                connector,
                config["parameters"],
                args.process,
                args.cand_gen,
                args.is_utilized,
                args.sel_oracle,
            )

            # return algorithm.get_index_candidates(workload, db_conf=db_conf, columns=columns)

            print("è®¡ç®—æœ€ä½³ç´¢å¼•...")
            if not args.process and not args.overhead:
                sel_info = ""
                indexes = algorithm.calculate_best_indexes(
                    workload, overhead=args.overhead, db_conf=db_conf, columns=columns
                )
            else:
                indexes, sel_info = algorithm.calculate_best_indexes(
                    workload, overhead=args.overhead, db_conf=db_conf, columns=columns
                )

            print(f"æ‰¾åˆ° {len(indexes)} ä¸ªæ¨èç´¢å¼•")

            indexes = [str(ind) for ind in indexes]
            cols = [ind.split(",") for ind in indexes]
            cols = [list(map(lambda x: x.split(".")[-1], col)) for col in cols]
            indexes = [
                f"{ind.split('.')[0]}#{','.join(col)}"
                for ind, col in zip(indexes, cols)
            ]
            indexes_res = [
                f"{ind.split('.')[0]}({','.join(col)})"
                for ind, col in zip(indexes, cols)
            ]

            if indexes:
                print(f"æ¨èçš„ç´¢å¼•: {indexes}")
            else:
                print("æœªæ‰¾åˆ°æ¨èç´¢å¼•")

            no_cost, ind_cost = list(), list()
            total_no_cost, total_ind_cost = 0, 0

            # # (0916): newly added.
            # freq_list = [1 for _ in work_list]
            # if isinstance(work_list[0], list):
            #     work_list = [item[1] for item in work_list]
            #     if args.varying_frequencies:
            #         freq_list = [item[-1] for item in work_list]
            #
            # # (0916): newly modified.
            # for sql, freq in zip(work_list, freq_list):
            #     no_cost_ = connector.get_ind_cost(sql, "") * freq
            #     total_no_cost += no_cost_
            #     no_cost.append(no_cost_)
            #
            #     ind_cost_ = connector.get_ind_cost(sql, indexes) * freq
            #     total_ind_cost += ind_cost_
            #     ind_cost.append(ind_cost_)

            # (0916): newly modified.
            print("è®¡ç®—æŸ¥è¯¢æˆæœ¬...")
            freq_list = list()
            for query_idx, query in enumerate(workload.queries, 1):
                if query_idx % 10 == 0 or query_idx == len(workload.queries):
                    print(f"æ­£åœ¨è®¡ç®—æŸ¥è¯¢ {query_idx}/{len(workload.queries)} çš„æˆæœ¬")

                no_cost_ = connector.get_ind_cost(query.text, "") * query.frequency
                total_no_cost += no_cost_
                no_cost.append(no_cost_)

                ind_cost_ = (
                    connector.get_ind_cost(query.text, indexes) * query.frequency
                )
                total_ind_cost += ind_cost_
                ind_cost.append(ind_cost_)

                freq_list.append(query.frequency)

            cost_reduction = total_no_cost - total_ind_cost
            cost_reduction_pct = (
                (cost_reduction / total_no_cost * 100) if total_no_cost > 0 else 0
            )
            print(
                f"æˆæœ¬åˆ†æå®Œæˆ - æ— ç´¢å¼•æ€»æˆæœ¬: {total_no_cost:.2f}, æœ‰ç´¢å¼•æ€»æˆæœ¬: {total_ind_cost:.2f}"
            )
            print(f"æˆæœ¬é™ä½: {cost_reduction:.2f} ({cost_reduction_pct:.2f}%)")

            # (0916): newly added.
            if args.varying_frequencies:
                data.append(
                    {
                        "config": config["parameters"],
                        "workload": [work_list, freq_list],
                        "indexes": indexes_res,
                        "no_cost": no_cost,
                        "total_no_cost": total_no_cost,
                        "ind_cost": ind_cost,
                        "total_ind_cost": total_ind_cost,
                        "sel_info": sel_info,
                    }
                )
            else:
                data.append(
                    {
                        "config": config["parameters"],
                        "workload": work_list,
                        "indexes": indexes_res,
                        "no_cost": no_cost,
                        "total_no_cost": total_no_cost,
                        "ind_cost": ind_cost,
                        "total_ind_cost": total_ind_cost,
                        "sel_info": sel_info,
                    }
                )

        if len(data) == 1:
            data = data[0]

        res_data[algo] = data
        print(f"ç®—æ³• {algo} å¤„ç†å®Œæˆ")

    print("æ‰€æœ‰ç®—æ³•å¤„ç†å®Œæˆ")
    return res_data


if __name__ == "__main__":
    print("=== å¯åŠ¨ç´¢å¼•å»ºè®®å™¨ç¨‹åº ===")

    parser = get_parser()
    args = parser.parse_args()

    algos = [args.algo] if args.algo else ALGORITHMS.keys()
    print(f"ä½¿ç”¨ç®—æ³•: {algos}")

    args.constraint = "storage"
    args.budget_MB = 500
    print(f"çº¦æŸæ¡ä»¶: {args.constraint}, å­˜å‚¨é¢„ç®—: {args.budget_MB} MB")

    # args.constraint = "number"
    args.max_indexes = 5
    print(f"æœ€å¤§ç´¢å¼•æ•°: {args.max_indexes}")

    args.multi_column = True
    print(f"æ”¯æŒå¤šåˆ—ç´¢å¼•: {args.multi_column}")

    print(f"è¯»å–å·¥ä½œè´Ÿè½½æ–‡ä»¶: {args.work_file}")
    work_list = None
    if args.work_file.endswith(".sql"):
        with open(args.work_file, "r") as rf:
            work_list = rf.readlines()
        print(f"ä»SQLæ–‡ä»¶è¯»å–äº† {len(work_list)} è¡ŒæŸ¥è¯¢")
    elif args.work_file.endswith(".json"):
        with open(args.work_file, "r") as rf:
            work_list = json.load(rf)
        print(f"ä»JSONæ–‡ä»¶è¯»å–äº† {len(work_list)} ä¸ªå·¥ä½œè´Ÿè½½")

    if work_list is None:
        print("æœªèƒ½è¯»å–å·¥ä½œè´Ÿè½½æ–‡ä»¶")
        exit(1)

    datas = []
    for work_idx, work in enumerate(work_list, 1):
        print(f"=== å¤„ç†å·¥ä½œè´Ÿè½½ {work_idx}/{len(work_list)} ===")
        print(f"å·¥ä½œè´Ÿè½½å†…å®¹: {work}")
        if isinstance(work, str):
            work = [work]
        data = get_heu_result(args, algos, work)
        print(f"å·¥ä½œè´Ÿè½½ {work_idx} å¤„ç†å®Œæˆ")
        print(f"ç»“æœ: {data}")
        datas.append(data)

    print(f"æ‰€æœ‰ {len(work_list)} ä¸ªå·¥ä½œè´Ÿè½½å¤„ç†å®Œæˆ")

    if args.res_save is not None:
        print(f"ä¿å­˜ç»“æœåˆ°æ–‡ä»¶: {args.res_save}")
        with open(args.res_save, "w") as wf:
            json.dump(datas, wf, indent=2, cls=IndexEncoder)
        print("ç»“æœä¿å­˜å®Œæˆ")
    else:
        print("æœªæŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œç»“æœæœªä¿å­˜")

    print("=== ç¨‹åºæ‰§è¡Œå®Œæˆ ===")
